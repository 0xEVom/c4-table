#!/usr/bin/env python3
import subprocess
from urllib.parse import urlparse
import re
import argparse
from argparse import RawTextHelpFormatter
import json
import concurrent.futures
import itertools
from collections import defaultdict

# %% Argument Handler
# Parse arguments
parser = argparse.ArgumentParser(
    description="Generate markdown table of issues from repo link and username on code4rena",
    epilog="Example: c4-table 2024-02-hydradx carrotsmuggler\n"
    "Please make sure gh (github cli) is installed and logged in with access to code4rena backstage",
    formatter_class=RawTextHelpFormatter,
)
parser.add_argument(
    "repo_link",
    help="Link to the repo. Accepts one of the following:\n"
    " 1. http link to the findings repo\n"
    " 2. Name of the contest repo\n"
    " 3. Name of the findings repo",
)
parser.add_argument(
    "username",
    help="case sensitive username to search for",
)
parser.add_argument(
    "-v", "--verbose", help="increase output verbosity", action="store_true"
)
parser.add_argument(
    "-u", "--url_only", help="Only lists html links, very fast", action="store_true"
)
args = parser.parse_args()


# %% Helper functions
# Fetch issues
def fetch_issues(page_num, responses_per_page):
    command = f'gh api -H "Accept: application/vnd.github+json" -H "X-GitHub-Api-Version: 2022-11-28" "/repos/{owner}/{repo}/issues?state=all&page={page_num}&per_page={responses_per_page}"'
    process = subprocess.Popen(
        command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE
    )
    stdout, stderr = process.communicate()
    if process.returncode != 0:
        print(f"Error grabbing issue data. Querying: {command} \n {stderr.decode()}")
    else:
        if args.verbose:
            print(f"Success getting issue list page: {page_num}")
    # Decode stdout from bytes to string
    stdout_str = stdout.decode("utf-8")
    # Strip ANSI escape codes
    ansi_escape = re.compile(r"\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])")
    stdout_str = ansi_escape.sub("", stdout_str)
    # Convert string to JSON
    return json.loads(stdout_str)


def print_table(
    numbers=[], urls=[], titles=[], status=[], dupes=[], username="", repo=""
):
    # Find the largest length
    number = max(len(numbers), len(urls), len(titles), len(status), len(dupes))
    combined_table = [
        "|" + "|".join(f" {str(el)} " if el else "" for el in elements) + "||"
        for elements in itertools.zip_longest(
            numbers, urls, titles, status, dupes, fillvalue=""
        )
    ]
    combined_table.insert(
        0, "| Num | URL | Title | Acceptance | Duplicate | Comments |"
    )
    combined_table.insert(1, "| :-: | :-: | :-: | :-: | :-: | :-: |")
    if args.verbose:
        print("\n")
    print("\n".join(combined_table))
    if number > 0:
        print(f"\nFound {number} issues from {username} in {repo} repo\n")


# %% Fetch file list and issue numbers
# Parse repo link
http_url = args.repo_link
username = args.username
if http_url[0] == "2":
    owner = "code-423n4"
    if http_url.endswith("findings"):
        repo = http_url
    else:
        repo = http_url + "-findings"
else:
    # Parse the URL
    parsed_url = urlparse(http_url)
    # Extract the owner and repo
    path_parts = parsed_url.path.strip("/").split("/")
    owner = path_parts[0]
    repo = path_parts[1]

# gh command. Ensure gh is installed and logged in
command = (
    f"gh api '/repos/{owner}/{repo}/git/trees/main?recursive=true' -q '.tree[]|.path'"
)
if args.verbose:
    print(command)
process = subprocess.Popen(
    command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE
)
stdout, stderr = process.communicate()
if process.returncode != 0:
    print(
        f"Error occurred: Is gh cli installed and logged in with correct user? \n {stderr.decode()}"
    )
else:
    if args.verbose:
        print(f"Success getting file list")
result = stdout.decode().strip().split("\n")

# Remove non HM
filtered_elements = [
    element
    for element in result
    if not element.endswith(("Analysis.md", "Q.md", "G.md", "README.md"))
]
issues_count = len(filtered_elements)

issuenum_by_user = defaultdict(list)

# Iterate over the filtered elements
for element in filtered_elements:
    # Use a regular expression to match the username and issue number
    match = re.match(r"data/(?P<username>.+)-(?P<number>\d+)\.json", element)
    if match:
        # Extract the username and issue number from the match
        temp_username = match.group("username")
        temp_num = int(match.group("number"))
        # Add the issue number to the list for this username
        issuenum_by_user[temp_username].append(temp_num)

# Sort the issue numbers for each user
for numbers in issuenum_by_user.values():
    numbers.sort()


# %% Only links (No issue parsing)
if args.url_only:
    issue_numbers = issuenum_by_user[username]
    url_list = [
        f"https://github.com/{owner}/{repo}/issues/{element}"
        for element in issue_numbers
    ]
    print_table(numbers=issue_numbers, urls=url_list, username=username, repo=repo)
    exit()

# %% Fetch issue details
# Get paginated issue details
responses_per_page = 100
pages = (issues_count // responses_per_page) + 1
issues_list = []
with concurrent.futures.ThreadPoolExecutor() as executor:
    futures = [
        executor.submit(fetch_issues, page_num, responses_per_page)
        for page_num in range(1, pages + 1)
    ]
    results = [future.result() for future in concurrent.futures.as_completed(futures)]
# Combine results into issues_list
for result in results:
    issues_list.extend(result)
issues_list.sort(key=lambda issue: issue["number"])

# convert issues into an array
issues_arr = [None] * (len(issues_list) + 10)
for issue in issues_list:
    issues_arr[issue["number"]] = issue

# %% User specific data
# Get relevant issues
issue_numbers = issuenum_by_user[username]
url_list = [
    element["html_url"] for element in issues_list if element["number"] in issue_numbers
]
title_list = [
    element["title"] for element in issues_list if element["number"] in issue_numbers
]
num_list = [
    element["number"] for element in issues_list if element["number"] in issue_numbers
]

# %% Table output
print_table(
    numbers=num_list, urls=url_list, titles=title_list, username=username, repo=repo
)
